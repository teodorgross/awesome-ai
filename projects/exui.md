---
title: "ExUI"
category: "Chat"
description: "Simple, lightweight browser-based UI for local inference using ExLlamaV2 with EXL2, GPTQ, and FP16 model support"
website: "https://github.com/turboderp/exui"
icon: "https://raw.githubusercontent.com/turboderp-org/exui/refs/heads/master/static/gfx/avatar_unicorn.png"
tags: ["exllamav2", "lightweight", "local-inference", "minimalistic", "fast"]
pricing: "Free"
---

# ExUI

A simple, lightweight browser-based user interface designed specifically for running local inference using ExLlamaV2. Focuses on minimalistic design, responsive performance, and essential features for efficient local LLM interaction.

## Key Features

**Lightweight Design**
- Minimalistic and responsive UI
- Fast loading and low resource overhead
- Browser-based interface with clean aesthetics
- Optimized for performance and simplicity

**ExLlamaV2 Integration**
- Built specifically for ExLlamaV2 backend
- Optimized for high-performance local inference
- Direct integration with ExLlama's optimization features
- Efficient memory usage and processing

**Multiple Model Support**
- EXL2 quantized models
- GPTQ quantized models  
- FP16 full-precision models
- Flexible model loading and switching

**Advanced Features**
- Persistent sessions across browser restarts
- Multiple instruct formats support
- Speculative decoding capabilities
- Notepad mode for extended writing

## Technical Specifications

- **Backend**: ExLlamaV2 inference
